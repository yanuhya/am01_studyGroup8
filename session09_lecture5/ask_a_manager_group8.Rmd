```{r}
library(googlesheets4)
library(tidyverse)
library(janitor) 
library(skimr)
library(countrycode) # to clean up country names
library(broom)
library(car)
library(ggfortify)
library(quantmod)
library(maps)
library(infer)
```


# Clean data

Load the ask a manager data set

```{r}
ask_a_manager_2021 <- read_csv(here::here("data", "ask_a_manager_2021.csv"))
```
Skim the data set to see if there are any missing or duplicate values

```{r}
skimr::skim(ask_a_manager_2021)
```

We see that the complete rate of "other_monetary_comp" and "currency_other" is 0. Hence, we can get rid of these columns.

```{r}
ask_a_manager_2021 <- ask_a_manager_2021 %>%
  select( -other_monetary_comp, - currency_other)
```

Let us also filter out the data where industry, highest level of educatin, gender and race is NA

```{r}
ask_a_manager_2021 <- ask_a_manager_2021 %>%
  filter(!is.na(industry) & !is.na(highest_level_of_education_completed) & !is.na(gender) & !is.na(race))
```

## Country

Now, the only columns where we have missing data is additional_context_on_job_title, additional_context_on_income, state and city

```{r warning=FALSE, message=FALSE}
unique_country_values <- unique(ask_a_manager_2021$country) %>%
  as_tibble()

unique_country_values$country_code = NA


for(i in 1:nrow(unique_country_values)){
  countryToTidy = unique_country_values$value[i]
  unique_country_values$country_code[i] = countrycode(countryToTidy, origin = 'country.name', destination = 'iso3c')
}

```

Then, find misspelled countries that has frequency higher 5 and clean the data.

```{r}
misspelled_countries <- ask_a_manager_2021[!(ask_a_manager_2021$country %in% (unique_country_values %>%
                                                      na.omit() %>%
                                                      select(value) %>% 
                                                      pull())),]

country_list <- misspelled_countries %>%
  count(country, sort=TRUE) %>%
  filter(n >= 5) %>%
  select(country) %>% 
  pull()

misspelled_countries <- misspelled_countries[misspelled_countries$country %in% country_list,]

misspelled_countries <- misspelled_countries %>%
  select(country) %>%
  unique() %>%
  mutate(country_code = ifelse(country == "Scotland" |
                                 country == "England", "GBR", ifelse(country == "NZ", "NZL", "USA"
                                 ))) %>%
  rbind((unique_country_values %>% na.omit() %>% rename(country = value)))

ask_a_manager_2021 <- ask_a_manager_2021 %>%
  left_join(misspelled_countries, by="country") %>%
  drop_na(country_code)
```

## Currency

```{r dealing with currenies}
from <- c("USD", "GBP", "CAD", "EUR", "AUD", "CHF", "ZAR", "SEK", "HKD", "JPY")
to <- "USD"
currency_rate <- rownames_to_column(getQuote(paste0(from, to, "=X")), "currency")[,c(1,3)] %>%
  mutate(currency = str_replace(currency, "USD=X", ""))
currency_rate[currency_rate$currency == "AUD","currency"] <- "AUD/NZD"

ask_a_manager_2021 <- left_join(ask_a_manager_2021, currency_rate, by="currency") %>%
  mutate(USD_salary = annual_salary * Last)

# We have 121 observations that has unsure currency, to let it not mess with our income variable (important variable), we decide to remove them entire.
ask_a_manager_2021 <- ask_a_manager_2021[!(is.na(ask_a_manager_2021$USD_salary)),]
```

# EDA

## Race

```{r}
race_list <- ask_a_manager_2021 %>%
  count(race, sort=TRUE) %>% 
  filter(n >= 100) %>% 
  select(race) %>% 
  pull()

ask_a_manager_2021_race <- ask_a_manager_2021[ask_a_manager_2021$race %in% race_list,]
```

## Industry

```{r}
industry_list <- ask_a_manager_2021 %>%
  count(industry, sort=TRUE) %>% 
  filter(n >= 100) %>% 
  select(industry) %>% 
  pull()

ask_a_manager_2021_industry <- ask_a_manager_2021[ask_a_manager_2021$industry %in% industry_list,]
```

## State

As our data is largely concentrated around employees working in the US, we want to investigate further into the differences of salary distribution within the US demographically. To clean the state variable, we only chose entries that have a frequency higher than 10, which leaves us with 50 states and District of Columbia. During this process, we only removed 100 entries in a total of 21562 entries, therefore we can assume that we did not lose any generality during the cleaning.

```{r}
# Remove all observations that has NA state, i.e. observations out of US
ask_a_manager_2021_state <- ask_a_manager_2021[!(is.na(ask_a_manager_2021$state)),]

# Only include observations with entries higher than 10
state_list <- ask_a_manager_2021_state %>%
  dplyr::count(state, sort=TRUE) %>% 
  filter(n >= 10) %>% 
  select(state) %>% 
  pull()
ask_a_manager_2021_state <- ask_a_manager_2021[ask_a_manager_2021$state %in% state_list,]

```

```{r}
ask_a_manager_2021_state_salary <- ask_a_manager_2021_state %>%
  group_by(state) %>% 
  summarise(mean_salary = mean(USD_salary))

ask_a_manager_2021_state_salary$state <- tolower(ask_a_manager_2021_state_salary$state)

# US Map data
MainStates <- map_data("state")
MainStates <- MainStates %>% 
  rename(state = region)

ask_a_manager_2021_state_salary <- ask_a_manager_2021_state_salary %>% 
  inner_join(MainStates, by="state")
```

The map below shows the mean salary for each states in the Mainland US.

```{r}
ggplot(ask_a_manager_2021_state_salary) + 
  geom_polygon(aes(x = long, y = lat, group = group, fill = mean_salary),
               color = "white", size = 0.2) +
  scale_fill_continuous(name="Mean Salary", 
            low = "white", high = "darkblue", limits = c(50000,120000), 
            na.value = "grey50") +
  theme_bw() +
  theme(axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        legend.key.size = unit(5, 'mm'),
        legend.title = element_text(size = 10, face = "bold"), 
        legend.text = element_text(size = 7),
        panel.background=element_blank(),
        panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        plot.background=element_blank(),
        plot.title = element_text(size = 15, face = "bold")) +
  labs(title = "Mean salary for each states in the Mainland US") +
  NULL
```

We can observe from the map that coastal states have a much darker color than the inland states. We will then do a t-test to see if the difference in salary between coastal states and inland states.

```{r}
# Identify costal states (Sourced from Wikipedia)
coastal_states <- c("alaska","california","hawaii","oregon","washington", "maine", "new hampshire", "massachusetts", "rhode island", "connecticut", "new york", "new jersey", "delaware", "maryland", "virginia", "north carolina", "south carolina", "georgia", "florida")

ask_a_manager_2021_state$state <- tolower(ask_a_manager_2021_state$state)

ask_a_manager_2021_state <- ask_a_manager_2021_state %>% 
  mutate(coastal_state = ifelse(state %in% coastal_states, "Yes", "No"))

# T-test to test for the significance of the differences
t.test(USD_salary ~ coastal_state, data = ask_a_manager_2021_state)

```

In this t-test, we have a t-statistic of up to 17.267 and a p-value close to 0. We are able to conclude that there is a difference in the salary between inland and coastal states. Demographically, this makes sense as coastal states tends to be more developed due to access to ports for international trading. This will lead to further economic development and hence the higher pay in these regions.




